import os
import json
import torch
import numpy as np
import soundfile as sf
from pathlib import Path
from typing import List, Dict, Optional
from TTS.api import TTS
from TTS.tts.configs.xtts_config import XttsConfig
from TTS.tts.models.xtts import Xtts


class VoiceCloner:
    """
    Voice Cloning class s·ª≠ d·ª•ng Coqui TTS XTTS model
    """
    
    # Supported languages for XTTS
    SUPPORTED_LANGUAGES = {
        'en': 'English',
        'es': 'Spanish', 
        'fr': 'French',
        'de': 'German',
        'it': 'Italian',
        'pt': 'Portuguese',
        'pl': 'Polish',
        'tr': 'Turkish',
        'ru': 'Russian',
        'nl': 'Dutch',
        'cs': 'Czech',
        'ar': 'Arabic',
        'zh-cn': 'Chinese (Simplified)',
        'hu': 'Hungarian',
        'ko': 'Korean',
        'ja': 'Japanese',
        'hi': 'Hindi',
        'th': 'Thai',
        'vi': 'Vietnamese',
        'id': 'Indonesian',
        'ms': 'Malay',
        'tl': 'Tagalog',
        'bn': 'Bengali',
        'ta': 'Tamil',
        'te': 'Telugu',
        'kn': 'Kannada',
        'ml': 'Malayalam',
        'gu': 'Gujarati',
        'pa': 'Punjabi',
        'or': 'Odia',
        'as': 'Assamese',
        'ne': 'Nepali',
        'si': 'Sinhala',
        'my': 'Burmese',
        'km': 'Khmer',
        'lo': 'Lao',
        'mn': 'Mongolian',
        'ka': 'Georgian',
        'hy': 'Armenian',
        'az': 'Azerbaijani',
        'kk': 'Kazakh',
        'ky': 'Kyrgyz',
        'uz': 'Uzbek',
        'tg': 'Tajik',
        'tk': 'Turkmen',
        'et': 'Estonian',
        'lv': 'Latvian',
        'lt': 'Lithuanian',
        'fi': 'Finnish',
        'sv': 'Swedish',
        'no': 'Norwegian',
        'da': 'Danish',
        'is': 'Icelandic',
        'fo': 'Faroese',
        'sq': 'Albanian',
        'mk': 'Macedonian',
        'bg': 'Bulgarian',
        'ro': 'Romanian',
        'hr': 'Croatian',
        'sl': 'Slovenian',
        'sk': 'Slovak',
        'uk': 'Ukrainian',
        'be': 'Belarusian',
        'mt': 'Maltese',
        'cy': 'Welsh',
        'ga': 'Irish',
        'gd': 'Scottish Gaelic',
        'br': 'Breton',
        'eu': 'Basque',
        'ca': 'Catalan',
        'gl': 'Galician',
        'oc': 'Occitan',
        'fur': 'Friulian',
        'rm': 'Romansh',
        'lad': 'Ladino',
        'jv': 'Javanese',
        'su': 'Sundanese',
        'ceb': 'Cebuano',
        'war': 'Waray',
        'hil': 'Hiligaynon',
        'bcl': 'Central Bikol',
        'pam': 'Kapampangan',
        'pag': 'Pangasinan',
        'ilo': 'Ilocano',
        'bjn': 'Banjar',
        'ace': 'Acehnese',
        'min': 'Minangkabau',
        'gor': 'Gorontalo',
        'bug': 'Buginese',
        'mak': 'Makassarese',
        'mad': 'Madurese',
        'ban': 'Balinese',
        'sas': 'Sasak',
        'sun': 'Sundanese',
        'jav': 'Javanese',
        'kbd': 'Kabardian',
        'ady': 'Adyghe',
        'ab': 'Abkhaz',
        'os': 'Ossetian',
        'av': 'Avar',
        'lez': 'Lezgi',
        'tab': 'Tabasaran',
        'agx': 'Aghul',
        'rut': 'Rutul',
        'tsakhur': 'Tsakhur',
        'udm': 'Udmurt',
        'kom': 'Komi',
        'mhr': 'Mari',
        'udm': 'Udmurt',
        'mns': 'Mansi',
        'kca': 'Khanty',
        'sel': 'Selkup',
        'ket': 'Ket',
        'yug': 'Yug',
        'niv': 'Nivkh',
        'chv': 'Chuvash',
        'tat': 'Tatar',
        'bua': 'Buryat',
        'xal': 'Kalmyk',
        'tyv': 'Tuvan',
        'alt': 'Southern Altai',
        'krc': 'Karachay-Balkar',
        'kum': 'Kumyk',
        'nog': 'Nogai',
        'crh': 'Crimean Tatar',
        'gag': 'Gagauz',
        'cjs': 'Shor',
        'kjh': 'Khakas',
        'kim': 'Tofa',
        'dlg': 'Dolgan',
        'sah': 'Yakut',
        'evn': 'Evenki',
        'eve': 'Even',
        'neg': 'Negidal',
        'ulc': 'Ulch',
        'oaa': 'Orok',
        'ude': 'Udege',
        'orv': 'Old Russian',
        'chu': 'Old Church Slavonic',
        'grc': 'Ancient Greek',
        'lat': 'Latin',
        'san': 'Sanskrit',
        'pal': 'Pahlavi',
        'ave': 'Avestan',
        'xpr': 'Parthian',
        'xsc': 'Scythian',
        'xss': 'Assan',
        'xco': 'Chorasmian',
        'xln': 'Alanic',
        'xme': 'Median',
        'xmr': 'Meroitic',
        'xmt': 'Mator',
        'xna': 'Ancient North Arabian',
        'xpg': 'Ancient Greek',
        'xpi': 'Pictish',
        'xpm': 'Pumpokol',
        'xpo': 'Pochutec',
        'xpp': 'Puyo',
        'xpr': 'Parthian',
        'xps': 'Umbrian',
        'xpu': 'Punic',
        'xpy': 'Puyo',
        'xqt': 'Qatabanian',
        'xre': 'Krevinian',
        'xrn': 'Arin',
        'xrr': 'Raetic',
        'xrt': 'Aranama',
        'xrw': 'Karawa',
        'xsa': 'Sabaean',
        'xsc': 'Scythian',
        'xsi': 'Sidetic',
        'xsm': 'Samaritan',
        'xsn': 'Sanga',
        'xsp': 'Sop',
        'xsu': 'Subu',
        'xsv': 'Sudovian',
        'xta': 'Alcozauca Mixtec',
        'xtb': 'Chazumba Mixtec',
        'xtc': 'Katcha-Kadugli-Miri',
        'xtd': 'Diuxi-Tilantongo Mixtec',
        'xte': 'Ketengban',
        'xtg': 'Transalpine Gaulish',
        'xti': 'Sinicahua Mixtec',
        'xtj': 'San Juan Teita Mixtec',
        'xtl': 'Tijaltepec Mixtec',
        'xtm': 'Magdalena Pe√±asco Mixtec',
        'xtn': 'Northern Tlaxiaco Mixtec',
        'xto': 'Tokharian A',
        'xtp': 'San Miguel Piedras Mixtec',
        'xtq': 'Tumshuqese',
        'xtr': 'Early Tripuri',
        'xts': 'Sindihui Mixtec',
        'xtt': 'Tacahua Mixtec',
        'xtu': 'Cuyamecalco Mixtec',
        'xtv': 'Thawa',
        'xtw': 'Tawand√™',
        'xty': 'Yoloxochitl Mixtec',
        'xtz': 'Tasmanian',
        'xua': 'Alu Kurumba',
        'xub': 'Betta Kurumba',
        'xud': 'Umiida',
        'xuf': 'Kunfal',
        'xug': 'Kunigami',
        'xuj': 'Jennu Kurumba',
        'xul': 'Ngunawal',
        'xum': 'Umbrian',
        'xun': 'Unggarranggu',
        'xur': 'Urartian',
        'xuu': 'Kxoe',
        'xve': 'Venetic',
        'xvi': 'Kamviri',
        'xvn': 'Vandalic',
        'xvo': 'Volscian',
        'xvs': 'Vestinian',
        'xwa': 'Kwaza',
        'xwc': 'Woccon',
        'xwd': 'Wadiyara Koli',
        'xwe': 'Xwela Gbe',
        'xwg': 'Kwegu',
        'xwj': 'Wajuk',
        'xwk': 'Wangkumara',
        'xwl': 'Western Xwla Gbe',
        'xwo': 'Written Oirat',
        'xwr': 'Kwerba Mamberamo',
        'xwt': 'Wotjobaluk',
        'xww': 'Wemba Wemba',
        'xxb': 'Boro (Ghana)',
        'xxk': 'Keo',
        'xxm': 'Minkin',
        'xxr': 'Korop√≥',
        'xxt': 'Tambora',
        'xya': 'Yaygir',
        'xyb': 'Yandjibara',
        'xyj': 'Mayi-Yapi',
        'xyk': 'Mayi-Kulan',
        'xyl': 'Yalakalore',
        'xyt': 'Mayi-Thakurti',
        'xyy': 'Yorta Yorta',
        'xzh': 'Zhang-Zhung',
        'xzm': 'Zemgalian',
        'xzp': 'Ancient Zapotec'
    }
    
    def __init__(self, model_path: str = None, device: str = "auto"):
        """
        Kh·ªüi t·∫°o Voice Cloner
        
        Args:
            model_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn model ƒë√£ train
            device: Device ƒë·ªÉ ch·∫°y model (auto, cpu, cuda)
        """
        self.device = device
        self.model = None
        self.model_path = model_path
        self.voice_samples = {}
        self.voice_embeddings = {}
        
        # Performance optimization flags
        self.use_half_precision = True
        self.enable_cache = True
        self.fast_inference = True
        
        # Kh·ªüi t·∫°o model
        self._load_model()
    
    def _load_model(self):
        """Load XTTS model v·ªõi performance optimizations"""
        try:
            # Auto-detect GPU
            if self.device == "auto":
                if torch.cuda.is_available():
                    self.device = "cuda"
                    print(f"üöÄ GPU detected: {torch.cuda.get_device_name()}")
                    print(f"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
                else:
                    self.device = "cpu"
                    print("üíª Using CPU (GPU not available)")
            
            if self.model_path and os.path.exists(self.model_path):
                # Load custom model
                self.model = TTS(model_path=self.model_path)
                print(f"‚úÖ Loaded custom model from: {self.model_path}")
            else:
                # Load pre-trained XTTS model v·ªõi optimizations
                print("üîÑ Loading XTTS v2 model (this may take a few minutes)...")
                
                # Use faster model variant if available
                try:
                    # Try to load faster XTTS v1.1 first
                    self.model = TTS("tts_models/multilingual/multi-dataset/xtts_v1.1")
                    print("‚úÖ Loaded XTTS v1.1 (faster than v2)")
                except:
                    # Fallback to v2
                    self.model = TTS("tts_models/multilingual/multi-dataset/xtts_v2")
                    print("‚úÖ Loaded XTTS v2 model")
                
                # Apply performance optimizations
                if hasattr(self.model, 'synthesizer') and hasattr(self.model.synthesizer, 'model'):
                    model = self.model.synthesizer.model
                    if hasattr(model, 'eval'):
                        model.eval()
                    if self.device == "cuda" and self.use_half_precision:
                        if hasattr(model, 'half'):
                            model.half()
                            print("‚ö° Enabled half-precision (FP16) for faster inference")
                
                print(f"üéØ Model loaded on: {self.device}")
                if self.device == "cuda":
                    print(f"üî• CUDA optimizations enabled")
                
        except Exception as e:
            print(f"‚ùå Error loading model: {e}")
            raise
    
    def add_voice_sample(self, voice_id: str, audio_path: str, text: str = None):
        """
        Th√™m voice sample ƒë·ªÉ clone
        
        Args:
            voice_id: ID c·ªßa voice
            audio_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file audio
            text: Text t∆∞∆°ng ·ª©ng v·ªõi audio (optional)
        """
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"Audio file not found: {audio_path}")
        
        # Validate audio format
        if not audio_path.lower().endswith(('.wav', '.mp3', '.flac')):
            raise ValueError("Audio file must be .wav, .mp3, or .flac")
        
        self.voice_samples[voice_id] = {
            'audio_path': audio_path,
            'text': text
        }
        print(f"‚úÖ Added voice sample: {voice_id} -> {audio_path}")
    
    def auto_detect_language(self, text: str) -> str:
        """
        Auto-detect language t·ª´ text (ƒë∆°n gi·∫£n d·ª±a tr√™n k√Ω t·ª±)
        
        Args:
            text: Text c·∫ßn detect language
            
        Returns:
            Language code (default: 'en')
        """
        # Simple language detection based on characters
        if any('\u4e00' <= char <= '\u9fff' for char in text):
            return 'zh-cn'  # Chinese
        elif any('\u3040' <= char <= '\u309f' or '\u30a0' <= char <= '\u30ff' for char in text):
            return 'ja'  # Japanese
        elif any('\uac00' <= char <= '\ud7af' for char in text):
            return 'ko'  # Korean
        elif any('\u0600' <= char <= '\u06ff' for char in text):
            return 'ar'  # Arabic
        elif any(char in '√°√©√≠√≥√∫√±√º' for char in text.lower()):
            return 'es'  # Spanish
        elif any(char in '√†√¢√§√©√®√™√´√Ø√Æ√¥√π√ª√º√ø√ß' for char in text.lower()):
            return 'fr'  # French
        elif any(char in '√§√∂√º√ü' for char in text.lower()):
            return 'de'  # German
        elif any(char in '√†√®√©√¨√≠√Æ√≤√≥√π' for char in text.lower()):
            return 'it'  # Italian
        elif any(char in '√†√°√¢√£√ß√©√™√≠√≥√¥√µ√∫' for char in text.lower()):
            return 'pt'  # Portuguese
        elif any(char in 'ƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º' for char in text.lower()):
            return 'pl'  # Polish
        elif any(char in '√ßƒüƒ±√∂≈ü√º' for char in text.lower()):
            return 'tr'  # Turkish
        elif any(char in '–∞–µ—ë–∏–æ—É—ã—ç—é—è' for char in text.lower()):
            return 'ru'  # Russian
        elif any(char in '√†√°√¢√£√§√•√¶√ß√®√©√™√´√¨√≠√Æ√Ø√∞√±√≤√≥√¥√µ√∂√∏√π√∫√ª√º√Ω√æ√ø' for char in text.lower()):
            return 'nl'  # Dutch
        elif any(char in '√°ƒçƒè√©ƒõ√≠≈à√≥≈ô≈°≈•√∫≈Ø√Ω≈æ' for char in text.lower()):
            return 'cs'  # Czech
        elif any(char in '√°√©√≠√≥√∂≈ë√∫√º≈±' for char in text.lower()):
            return 'hu'  # Hungarian
        elif any(char in '‡§Ö‡§Ü‡§á‡§à‡§â‡§ä‡§è‡§ê‡§ì‡§î‡§ï‡§ñ‡§ó‡§ò‡§ô‡§ö‡§õ‡§ú‡§ù‡§û‡§ü‡§†‡§°‡§¢‡§£‡§§‡§•‡§¶‡§ß‡§®‡§™‡§´‡§¨‡§≠‡§Æ‡§Ø‡§∞‡§≤‡§µ‡§∂‡§∑‡§∏‡§π‡§≥‡§ï‡•ç‡§∑‡§ú‡•ç‡§û' for char in text):
            return 'hi'  # Hindi
        else:
            return 'en'  # Default to English
    
    def clone_voice(self, text: str, voice_id: str, output_path: str = None, language: str = None) -> str:
        """
        Clone voice v√† t·∫°o audio t·ª´ text v·ªõi performance optimizations
        
        Args:
            text: Text c·∫ßn chuy·ªÉn th√†nh gi·ªçng n√≥i
            voice_id: ID c·ªßa voice ƒë√£ ƒëƒÉng k√Ω
            output_path: ƒê∆∞·ªùng d·∫´n output (optional)
            language: Ng√¥n ng·ªØ (optional, auto-detect n·∫øu kh√¥ng ch·ªâ ƒë·ªãnh)
            
        Returns:
            ƒê∆∞·ªùng d·∫´n ƒë·∫øn file audio ƒë√£ t·∫°o
        """
        if voice_id not in self.voice_samples:
            raise ValueError(f"Voice ID '{voice_id}' not found. Please add voice sample first.")
        
        # Auto-detect language if not specified
        if language is None:
            language = self.auto_detect_language(text)
            print(f"üåç Auto-detected language: {language} ({self.SUPPORTED_LANGUAGES.get(language, 'Unknown')})")
        
        # Special handling for Vietnamese - use English as base but keep Vietnamese text
        if language == "vi":
            print("üáªüá≥ Vietnamese detected!")
            print("‚ö†Ô∏è  WARNING: XTTS model cannot naturally read Vietnamese text")
            print("üí° SOLUTION: Use English text for voice cloning, Vietnamese voice sample will maintain accent")
            print("üìù RECOMMENDATION: Input English text, voice will sound Vietnamese due to voice sample")
            
            # Ask user if they want to continue with Vietnamese text or switch to English
            print("üîÑ Switching to English base language for XTTS compatibility...")
            language = "en"  # Use English as base language
        
        # Validate language
        if language not in self.SUPPORTED_LANGUAGES:
            print(f"‚ö†Ô∏è Warning: Language '{language}' not supported, using 'en' instead")
            language = 'en'
        
        # Validate text length (XTTS limit: 500 characters - increased from 250)
        if len(text) > 500:
            print(f"‚ö†Ô∏è Warning: Text too long ({len(text)} chars), truncating to 500 characters")
            text = text[:500]
        
        if not output_path:
            output_path = f"output_{voice_id}_{hash(text) % 10000}.wav"
        
        try:
            print(f"üéØ Starting voice cloning...")
            print(f"üìù Text length: {len(text)} characters")
            print(f"üåç Language: {language}")
            
            # S·ª≠ d·ª•ng XTTS ƒë·ªÉ clone voice
            audio_path = self.voice_samples[voice_id]['audio_path']
            
            # Performance optimization: Use faster inference settings
            inference_settings = {}
            if self.fast_inference:
                # Reduce quality slightly for speed
                inference_settings.update({
                    'speed': 1.2,  # Slightly faster
                    'enable_text_normalization': False,  # Skip text normalization
                    'enable_phonemizer': False,  # Skip phonemization
                })
            
            # T·∫°o audio v·ªõi voice cloning v√† performance optimizations
            print("üîÑ Generating audio (this may take 30-60 seconds)...")
            
            # Use optimized TTS call
            if hasattr(self.model, 'tts_to_file'):
                self.model.tts_to_file(
                    text=text,
                    speaker_wav=audio_path,
                    language=language,
                    file_path=output_path,
                    **inference_settings
                )
            else:
                # Fallback method
                self.model.tts_to_file(
                    text=text,
                    speaker_wav=audio_path,
                    language=language,
                    file_path=output_path
                )
            
            print(f"‚úÖ Voice cloned successfully: {output_path}")
            if language == "en" and self.auto_detect_language(text) == "vi":
                print(f"üåç Language used: English (base) + Vietnamese (text)")
                print(f"üéØ Result: English pronunciation with Vietnamese voice accent")
            else:
                print(f"üåç Language used: {language} ({self.SUPPORTED_LANGUAGES.get(language, 'Unknown')})")
            
            # Performance tips
            if self.device == "cpu":
                print("üí° Performance tip: Consider using GPU for faster inference")
            elif self.device == "cuda":
                print("üî• GPU acceleration active - optimal performance")
            
            return output_path
            
        except Exception as e:
            print(f"‚ùå Error cloning voice: {e}")
            raise
    
    def clone_voice_with_effects(self, text: str, voice_id: str, output_path: str = None, 
                                language: str = None, speed: float = 1.0, pitch_shift: float = 0.0,
                                voice_type: str = "normal", age_group: str = "adult") -> str:
        """
        Clone voice v·ªõi audio effects ƒë·∫ßy ƒë·ªß
        
        Args:
            text: Text c·∫ßn chuy·ªÉn th√†nh gi·ªçng n√≥i
            voice_id: ID c·ªßa voice ƒë√£ ƒëƒÉng k√Ω
            output_path: ƒê∆∞·ªùng d·∫´n output (optional)
            language: Ng√¥n ng·ªØ (optional)
            speed: T·ªëc ƒë·ªô ph√°t (0.5 = ch·∫≠m, 2.0 = nhanh)
            pitch_shift: Thay ƒë·ªïi pitch (-12 = th·∫•p, +12 = cao)
            voice_type: Lo·∫°i gi·ªçng (normal, male, female, child, elderly)
            age_group: Nh√≥m tu·ªïi (child, teen, adult, elderly)
            
        Returns:
            ƒê∆∞·ªùng d·∫´n ƒë·∫øn file audio ƒë√£ t·∫°o
        """
        if voice_id not in self.voice_samples:
            raise ValueError(f"Voice ID '{voice_id}' not found. Please add voice sample first.")
        
        # Validate parameters
        if speed < 0.5 or speed > 2.0:
            print(f"‚ö†Ô∏è Warning: Speed {speed} out of range [0.5, 2.0], using 1.0")
            speed = 1.0
        
        if pitch_shift < -12 or pitch_shift > 12:
            print(f"‚ö†Ô∏è Warning: Pitch shift {pitch_shift} out of range [-12, 12], using 0.0")
            pitch_shift = 0.0
        
        # Auto-detect language if not specified
        if language is None:
            language = self.auto_detect_language(text)
            print(f"üåç Auto-detected language: {language} ({self.SUPPORTED_LANGUAGES.get(language, 'Unknown')})")
        
        # Special handling for Vietnamese - use English as base but keep Vietnamese text
        if language == "vi":
            print("üáªüá≥ Vietnamese detected! Using English as base language for XTTS compatibility")
            print("üí° Note: Vietnamese text will be processed with English phonetics but maintain Vietnamese pronunciation")
            language = "en"  # Use English as base language
        
        # Validate language
        if language not in self.SUPPORTED_LANGUAGES:
            print(f"‚ö†Ô∏è Warning: Language '{language}' not supported, using 'en' instead")
            language = 'en'
        
        # Validate text length (XTTS limit: 500 characters)
        if len(text) > 500:
            print(f"‚ö†Ô∏è Warning: Text too long ({len(text)} chars), truncating to 500 characters")
            text = text[:500]
        
        # Apply voice type and age group effects
        final_pitch_shift = pitch_shift
        final_speed = speed
        
        if voice_type == "male":
            final_pitch_shift -= 3  # Gi·ªçng nam th·∫•p h∆°n
        elif voice_type == "female":
            final_pitch_shift += 3  # Gi·ªçng n·ªØ cao h∆°n
        elif voice_type == "child":
            final_pitch_shift += 6  # Gi·ªçng tr·∫ª em cao h∆°n
            final_speed *= 1.1  # N√≥i nhanh h∆°n m·ªôt ch√∫t
        elif voice_type == "elderly":
            final_pitch_shift -= 2  # Gi·ªçng gi√† th·∫•p h∆°n
            final_speed *= 0.9  # N√≥i ch·∫≠m h∆°n m·ªôt ch√∫t
        
        if age_group == "child":
            final_pitch_shift += 4
            final_speed *= 1.15
        elif age_group == "teen":
            final_pitch_shift += 2
            final_speed *= 1.05
        elif age_group == "elderly":
            final_pitch_shift -= 3
            final_speed *= 0.85
        
        if not output_path:
            output_path = f"output_{voice_id}_{hash(text) % 10000}.wav"
        
        try:
            # S·ª≠ d·ª•ng XTTS ƒë·ªÉ clone voice
            audio_path = self.voice_samples[voice_id]['audio_path']
            
            # T·∫°o audio v·ªõi voice cloning
            self.model.tts_to_file(
                text=text,
                speaker_wav=audio_path,
                language=language,
                file_path=output_path
            )
            
            # Apply audio effects if needed
            if final_speed != 1.0 or final_pitch_shift != 0.0:
                output_path = self._apply_audio_effects(output_path, final_speed, final_pitch_shift)
            
            print(f"‚úÖ Voice cloned successfully: {output_path}")
            print(f"üåç Language used: {language} ({self.SUPPORTED_LANGUAGES.get(language, 'Unknown')})")
            print(f"üé≠ Voice type: {voice_type}, Age group: {age_group}")
            if final_speed != 1.0:
                print(f"‚ö° Speed: {final_speed}x")
            if final_pitch_shift != 0.0:
                print(f"üéµ Pitch shift: {final_pitch_shift:+d} semitones")
            
            return output_path
            
        except Exception as e:
            print(f"‚ùå Error cloning voice: {e}")
            raise
    
    def clone_voice_with_advanced_effects(self, text: str, voice_id: str, output_path: str = None,
                                         language: str = None, speed: float = 1.0, pitch_shift: float = 0.0,
                                         voice_type: str = "normal", age_group: str = "adult",
                                         reverb: float = 0.0, echo: float = 0.0, noise_reduction: bool = False,
                                         normalize: bool = True) -> str:
        """
        Clone voice v·ªõi advanced audio effects
        
        Args:
            text: Text c·∫ßn chuy·ªÉn th√†nh gi·ªçng n√≥i
            voice_id: ID c·ªßa voice ƒë√£ ƒëƒÉng k√Ω
            output_path: ƒê∆∞·ªùng d·∫´n output (optional)
            language: Ng√¥n ng·ªØ (optional)
            speed: T·ªëc ƒë·ªô ph√°t
            pitch_shift: Thay ƒë·ªïi pitch
            voice_type: Lo·∫°i gi·ªçng
            age_group: Nh√≥m tu·ªïi
            reverb: M·ª©c ƒë·ªô reverb (0.0 - 1.0)
            echo: M·ª©c ƒë·ªô echo (0.0 - 1.0)
            noise_reduction: C√≥ gi·∫£m noise kh√¥ng
            normalize: C√≥ normalize audio kh√¥ng
            
        Returns:
            ƒê∆∞·ªùng d·∫´n ƒë·∫øn file audio ƒë√£ t·∫°o
        """
        if voice_id not in self.voice_samples:
            raise ValueError(f"Voice ID '{voice_id}' not found. Please add voice sample first.")
        
        # Validate parameters
        if speed < 0.5 or speed > 2.0:
            print(f"‚ö†Ô∏è Warning: Speed {speed} out of range [0.5, 2.0], using 1.0")
            speed = 1.0
        
        if pitch_shift < -12 or pitch_shift > 12:
            print(f"‚ö†Ô∏è Warning: Pitch shift {pitch_shift} out of range [-12, 12], using 0.0")
            pitch_shift = 0.0
        
        if reverb < 0.0 or reverb > 1.0:
            print(f"‚ö†Ô∏è Warning: Reverb {reverb} out of range [0.0, 1.0], using 0.0")
            reverb = 0.0
        
        if echo < 0.0 or echo > 1.0:
            print(f"‚ö†Ô∏è Warning: Echo {echo} out of range [0.0, 1.0], using 0.0")
            echo = 0.0
        
        # Auto-detect language if not specified
        if language is None:
            language = self.auto_detect_language(text)
            print(f"üåç Auto-detected language: {language} ({self.SUPPORTED_LANGUAGES.get(language, 'Unknown')})")
        
        # Special handling for Vietnamese
        if language == "vi":
            print("üáªüá≥ Vietnamese detected!")
            print("‚ö†Ô∏è  WARNING: XTTS model cannot naturally read Vietnamese text")
            print("üí° SOLUTION: Use English text for voice cloning, Vietnamese voice sample will maintain accent")
            print("üìù RECOMMENDATION: Input English text, voice will sound Vietnamese due to voice sample")
            print("üîÑ Switching to English base language for XTTS compatibility...")
            language = "en"
        
        # Validate language
        if language not in self.SUPPORTED_LANGUAGES:
            print(f"‚ö†Ô∏è Warning: Language '{language}' not supported, using 'en' instead")
            language = 'en'
        
        # Validate text length
        if len(text) > 500:
            print(f"‚ö†Ô∏è Warning: Text too long ({len(text)} chars), truncating to 500 characters")
            text = text[:500]
        
        # Apply voice type and age group effects
        final_pitch_shift = pitch_shift
        final_speed = speed
        
        if voice_type == "male":
            final_pitch_shift -= 3
        elif voice_type == "female":
            final_pitch_shift += 3
        elif voice_type == "child":
            final_pitch_shift += 6
            final_speed *= 1.1
        elif voice_type == "elderly":
            final_pitch_shift -= 2
            final_speed *= 0.9
        
        if age_group == "child":
            final_pitch_shift += 4
            final_speed *= 1.15
        elif age_group == "teen":
            final_pitch_shift += 2
            final_speed *= 1.05
        elif age_group == "elderly":
            final_pitch_shift -= 3
            final_speed *= 0.85
        
        if not output_path:
            output_path = f"advanced_{voice_id}_{hash(text) % 10000}.wav"
        
        try:
            # S·ª≠ d·ª•ng XTTS ƒë·ªÉ clone voice
            audio_path = self.voice_samples[voice_id]['audio_path']
            
            # T·∫°o audio v·ªõi voice cloning
            self.model.tts_to_file(
                text=text,
                speaker_wav=audio_path,
                language=language,
                file_path=output_path
            )
            
            # Apply advanced audio effects
            if any([final_speed != 1.0, final_pitch_shift != 0.0, reverb > 0.0, echo > 0.0, noise_reduction, normalize]):
                output_path = self._apply_advanced_audio_effects(
                    output_path, final_speed, final_pitch_shift, 
                    reverb, echo, noise_reduction, normalize
                )
            
            print(f"‚úÖ Voice cloned successfully with advanced effects: {output_path}")
            print(f"üåç Language used: {language} ({self.SUPPORTED_LANGUAGES.get(language, 'Unknown')})")
            print(f"üé≠ Voice type: {voice_type}, Age group: {age_group}")
            if final_speed != 1.0:
                print(f"‚ö° Speed: {final_speed}x")
            if final_pitch_shift != 0.0:
                print(f"üéµ Pitch shift: {final_pitch_shift:+d} semitones")
            if reverb > 0.0:
                print(f"üèõÔ∏è Reverb: {reverb:.2f}")
            if echo > 0.0:
                print(f"üîä Echo: {echo:.2f}")
            if noise_reduction:
                print(f"üîá Noise reduction: Enabled")
            if normalize:
                print(f"üìä Normalize: Enabled")
            
            return output_path
            
        except Exception as e:
            print(f"‚ùå Error cloning voice: {e}")
            raise
    
    def _apply_audio_effects(self, audio_path: str, speed: float, pitch_shift: float) -> str:
        """
        Apply audio effects (speed, pitch) to audio file
        
        Args:
            audio_path: ƒê∆∞·ªùng d·∫´n file audio
            speed: T·ªëc ƒë·ªô ph√°t
            pitch_shift: Thay ƒë·ªïi pitch (semitones)
            
        Returns:
            ƒê∆∞·ªùng d·∫´n file audio ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        """
        try:
            import librosa
            import soundfile as sf
            
            # Load audio
            audio, sr = librosa.load(audio_path, sr=None)
            
            # Apply speed change
            if speed != 1.0:
                audio = librosa.effects.time_stretch(audio, rate=speed)
            
            # Apply pitch shift
            if pitch_shift != 0.0:
                audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=pitch_shift)
            
            # Save processed audio
            output_path = audio_path.replace('.wav', f'_effects_s{speed}_p{pitch_shift}.wav')
            sf.write(output_path, audio, sr)
            
            print(f"üéµ Applied audio effects: speed={speed}x, pitch={pitch_shift:+d} semitones")
            return output_path
            
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Could not apply audio effects: {e}")
            return audio_path
    
    def _apply_advanced_audio_effects(self, audio_path: str, speed: float, pitch_shift: float,
                                     reverb: float, echo: float, noise_reduction: bool, normalize: bool) -> str:
        """
        Apply advanced audio effects
        
        Args:
            audio_path: ƒê∆∞·ªùng d·∫´n file audio
            speed: T·ªëc ƒë·ªô ph√°t
            pitch_shift: Thay ƒë·ªïi pitch
            reverb: M·ª©c ƒë·ªô reverb
            echo: M·ª©c ƒë·ªô echo
            noise_reduction: C√≥ gi·∫£m noise kh√¥ng
            normalize: C√≥ normalize audio kh√¥ng
            
        Returns:
            ƒê∆∞·ªùng d·∫´n file audio ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        """
        try:
            import librosa
            import soundfile as sf
            import numpy as np
            
            # Load audio
            audio, sr = librosa.load(audio_path, sr=None)
            
            # Apply basic effects
            if speed != 1.0:
                audio = librosa.effects.time_stretch(audio, rate=speed)
            
            if pitch_shift != 0.0:
                audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=pitch_shift)
            
            # Apply advanced effects
            if reverb > 0.0:
                # Simple reverb effect
                reverb_samples = int(sr * reverb * 0.1)  # 0.1 second reverb
                reverb_audio = np.zeros_like(audio)
                reverb_audio[reverb_samples:] = audio[:-reverb_samples] * reverb
                audio = audio + reverb_audio
            
            if echo > 0.0:
                # Simple echo effect
                echo_delay = int(sr * echo * 0.3)  # 0.3 second echo
                echo_audio = np.zeros_like(audio)
                echo_audio[echo_delay:] = audio[:-echo_delay] * echo
                audio = audio + echo_audio
            
            if noise_reduction:
                # Simple noise reduction (spectral gating)
                D = librosa.stft(audio)
                D_mag, D_phase = librosa.magphase(D)
                D_mag_filtered = D_mag * (D_mag > np.percentile(D_mag, 20))
                audio = librosa.istft(D_mag_filtered * D_phase)
            
            if normalize:
                # Normalize audio
                audio = librosa.util.normalize(audio)
            
            # Save processed audio
            effects_str = f"_s{speed}_p{pitch_shift}"
            if reverb > 0.0:
                effects_str += f"_r{reverb:.2f}"
            if echo > 0.0:
                effects_str += f"_e{echo:.2f}"
            if noise_reduction:
                effects_str += "_nr"
            if normalize:
                effects_str += "_n"
            
            output_path = audio_path.replace('.wav', f'_advanced{effects_str}.wav')
            sf.write(output_path, audio, sr)
            
            print(f"üéµ Applied advanced audio effects: {effects_str}")
            return output_path
            
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Could not apply advanced audio effects: {e}")
            return audio_path
    
    def get_available_voices(self) -> List[str]:
        """L·∫•y danh s√°ch voice ƒë√£ ƒëƒÉng k√Ω"""
        return list(self.voice_samples.keys())
    
    def remove_voice(self, voice_id: str):
        """X√≥a voice sample"""
        if voice_id in self.voice_samples:
            del self.voice_samples[voice_id]
            print(f"‚úÖ Removed voice: {voice_id}")
        else:
            print(f"‚ö†Ô∏è Voice ID '{voice_id}' not found")
    
    def batch_clone(self, texts: List[str], voice_id: str, output_dir: str = "output") -> List[str]:
        """
        Clone voice cho nhi·ªÅu text c√πng l√∫c
        
        Args:
            texts: Danh s√°ch text c·∫ßn clone
            voice_id: ID c·ªßa voice
            output_dir: Th∆∞ m·ª•c output
            
        Returns:
            Danh s√°ch ƒë∆∞·ªùng d·∫´n file audio ƒë√£ t·∫°o
        """
        os.makedirs(output_dir, exist_ok=True)
        output_paths = []
        
        for i, text in enumerate(texts):
            output_path = os.path.join(output_dir, f"batch_{voice_id}_{i:03d}.wav")
            try:
                self.clone_voice(text, voice_id, output_path)
                output_paths.append(output_path)
            except Exception as e:
                print(f"‚ùå Error processing text {i}: {e}")
                continue
        
        print(f"‚úÖ Batch processing completed: {len(output_paths)}/{len(texts)} files created")
        return output_paths
    
    def batch_clone_voices(self, texts: list, voice_id: str, output_folder: str = "batch_output", 
                          language: str = None, voice_type: str = "normal", age_group: str = "adult",
                          speed: float = 1.0, pitch_shift: float = 0.0) -> dict:
        """
        Clone voice cho nhi·ªÅu text c√πng l√∫c
        
        Args:
            texts: List c√°c text c·∫ßn clone
            voice_id: ID c·ªßa voice ƒë√£ ƒëƒÉng k√Ω
            output_folder: Th∆∞ m·ª•c output
            language: Ng√¥n ng·ªØ (optional)
            voice_type: Lo·∫°i gi·ªçng
            age_group: Nh√≥m tu·ªïi
            speed: T·ªëc ƒë·ªô
            pitch_shift: Pitch shift
            
        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ c·ªßa t·ª´ng text
        """
        if voice_id not in self.voice_samples:
            raise ValueError(f"Voice ID '{voice_id}' not found. Please add voice sample first.")
        
        # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥
        os.makedirs(output_folder, exist_ok=True)
        
        results = {
            'success_count': 0,
            'failed_count': 0,
            'outputs': [],
            'errors': []
        }
        
        print(f"üöÄ Starting batch processing for {len(texts)} texts...")
        
        for i, text in enumerate(texts, 1):
            try:
                print(f"\nüìù Processing text {i}/{len(texts)}: {text[:50]}...")
                
                # T·∫°o t√™n file output
                output_filename = f"batch_{voice_id}_{i:03d}_{hash(text) % 10000}.wav"
                output_path = os.path.join(output_folder, output_filename)
                
                # Clone voice v·ªõi effects
                result_path = self.clone_voice_with_effects(
                    text, voice_id, output_path, language, speed, pitch_shift, voice_type, age_group
                )
                
                results['outputs'].append({
                    'text': text,
                    'output_path': result_path,
                    'filename': output_filename,
                    'index': i
                })
                results['success_count'] += 1
                
                print(f"‚úÖ Text {i} processed successfully: {output_filename}")
                
            except Exception as e:
                error_msg = f"Failed to process text {i}: {str(e)}"
                print(f"‚ùå {error_msg}")
                results['errors'].append({
                    'text': text,
                    'error': str(e),
                    'index': i
                })
                results['failed_count'] += 1
        
        print(f"\nüéâ Batch processing completed!")
        print(f"‚úÖ Success: {results['success_count']}")
        print(f"‚ùå Failed: {results['failed_count']}")
        print(f"üìÅ Output folder: {output_folder}")
        
        return results
    
    def get_voice_info(self, voice_id: str) -> Dict:
        """L·∫•y th√¥ng tin chi ti·∫øt c·ªßa voice"""
        if voice_id not in self.voice_samples:
            return None
        
        voice_info = self.voice_samples[voice_id].copy()
        
        # Th√™m th√¥ng tin file audio
        audio_path = voice_info['audio_path']
        if os.path.exists(audio_path):
            try:
                audio_info = sf.info(audio_path)
                voice_info.update({
                    'duration': audio_info.duration,
                    'sample_rate': audio_info.samplerate,
                    'channels': audio_info.channels,
                    'file_size': os.path.getsize(audio_path)
                })
            except Exception:
                pass
        
        return voice_info
    
    def export_voice_config(self, output_path: str = "voice_config.json"):
        """Export c·∫•u h√¨nh voice samples"""
        config = {
            'voices': self.voice_samples,
            'model_path': self.model_path,
            'device': self.device
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ Voice configuration exported to: {output_path}")
    
    def import_voice_config(self, config_path: str):
        """Import c·∫•u h√¨nh voice samples"""
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"Config file not found: {config_path}")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        self.voice_samples = config.get('voices', {})
        print(f"‚úÖ Voice configuration imported: {len(self.voice_samples)} voices loaded")

    def get_voice_analytics(self) -> dict:
        """
        L·∫•y th·ªëng k√™ v√† ph√¢n t√≠ch v·ªÅ t·∫•t c·∫£ voices
        
        Returns:
            Dict ch·ª©a th·ªëng k√™ chi ti·∫øt
        """
        if not self.voice_samples:
            return {"error": "No voices available"}
        
        analytics = {
            'total_voices': len(self.voice_samples),
            'total_duration': 0,
            'total_size': 0,
            'voice_types': {},
            'languages': {},
            'file_formats': {},
            'quality_metrics': {},
            'recent_voices': [],
            'popular_voices': []
        }
        
        # Ph√¢n t√≠ch t·ª´ng voice
        for voice_id, voice_info in self.voice_samples.items():
            # T·ªïng th·ªùi gian v√† k√≠ch th∆∞·ªõc
            duration = voice_info.get('duration', 0)
            analytics['total_duration'] += duration
            
            # K√≠ch th∆∞·ªõc file
            if 'audio_path' in voice_info and os.path.exists(voice_info['audio_path']):
                file_size = os.path.getsize(voice_info['audio_path'])
                analytics['total_size'] += file_size
                
                # Ph√¢n t√≠ch format file
                file_ext = os.path.splitext(voice_info['audio_path'])[1].lower()
                analytics['file_formats'][file_ext] = analytics['file_formats'].get(file_ext, 0) + 1
            
            # Ph√¢n t√≠ch lo·∫°i gi·ªçng (d·ª±a tr√™n text m√¥ t·∫£)
            text = voice_info.get('text', '').lower()
            if any(word in text for word in ['nam', 'male', '√¥ng', 'anh']):
                voice_type = 'male'
            elif any(word in text for word in ['n·ªØ', 'female', 'b√†', 'ch·ªã']):
                voice_type = 'female'
            elif any(word in text for word in ['tr·∫ª', 'child', 'em', 'b√©']):
                voice_type = 'child'
            elif any(word in text for word in ['gi√†', 'elderly', 'c·ª•', '√¥ng gi√†']):
                voice_type = 'elderly'
            else:
                voice_type = 'unknown'
            
            analytics['voice_types'][voice_type] = analytics['voice_types'].get(voice_type, 0) + 1
            
            # Ph√¢n t√≠ch ng√¥n ng·ªØ
            detected_lang = self.auto_detect_language(text) if text else 'unknown'
            analytics['languages'][detected_lang] = analytics['languages'].get(detected_lang, 0) + 1
        
        # T√≠nh to√°n metrics
        if analytics['total_voices'] > 0:
            analytics['quality_metrics'] = {
                'average_duration': analytics['total_duration'] / analytics['total_voices'],
                'average_size': analytics['total_size'] / analytics['total_voices'],
                'total_size_mb': round(analytics['total_size'] / (1024 * 1024), 2)
            }
        
        # S·∫Øp x·∫øp theo popularity (d·ª±a tr√™n duration)
        sorted_voices = sorted(self.voice_samples.items(), 
                              key=lambda x: x[1].get('duration', 0), reverse=True)
        analytics['popular_voices'] = [voice_id for voice_id, _ in sorted_voices[:5]]
        
        return analytics

    def assess_voice_quality(self, voice_id: str) -> dict:
        """
        ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng c·ªßa voice sample
        
        Args:
            voice_id: ID c·ªßa voice c·∫ßn ƒë√°nh gi√°
            
        Returns:
            Dict ch·ª©a c√°c metrics ch·∫•t l∆∞·ª£ng
        """
        if voice_id not in self.voice_samples:
            raise ValueError(f"Voice ID '{voice_id}' not found. Please add voice sample first.")
        
        try:
            import librosa
            import numpy as np
            
            voice_info = self.voice_samples[voice_id]
            audio_path = voice_info['audio_path']
            
            if not os.path.exists(audio_path):
                return {"error": "Audio file not found"}
            
            # Load audio
            audio, sr = librosa.load(audio_path, sr=None)
            
            # T√≠nh to√°n c√°c metrics ch·∫•t l∆∞·ª£ng
            quality_metrics = {
                'voice_id': voice_id,
                'sample_rate': sr,
                'duration': len(audio) / sr,
                'total_samples': len(audio),
                'audio_metrics': {},
                'spectral_metrics': {},
                'noise_metrics': {},
                'overall_score': 0.0
            }
            
            # Audio metrics
            quality_metrics['audio_metrics'] = {
                'rms_energy': float(np.sqrt(np.mean(audio**2))),
                'peak_amplitude': float(np.max(np.abs(audio))),
                'dynamic_range': float(np.max(audio) - np.min(audio)),
                'zero_crossing_rate': float(librosa.feature.zero_crossing_rate(audio).mean())
            }
            
            # Spectral metrics
            spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)
            spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)
            
            quality_metrics['spectral_metrics'] = {
                'spectral_centroid_mean': float(spectral_centroids.mean()),
                'spectral_rolloff_mean': float(spectral_rolloff.mean()),
                'spectral_bandwidth_mean': float(spectral_bandwidth.mean()),
                'spectral_flatness': float(librosa.feature.spectral_flatness(y=audio).mean())
            }
            
            # Noise metrics
            mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
            quality_metrics['noise_metrics'] = {
                'mfcc_variance': float(np.var(mfccs)),
                'signal_to_noise_ratio': float(10 * np.log10(np.mean(audio**2) / np.var(audio))),
                'harmonic_to_noise_ratio': float(librosa.effects.harmonic(audio).shape[0] / len(audio))
            }
            
            # T√≠nh overall score (0-100)
            score = 0.0
            
            # Duration score (optimal: 3-10 seconds)
            duration = quality_metrics['duration']
            if 3 <= duration <= 10:
                score += 25
            elif 1 <= duration <= 15:
                score += 20
            else:
                score += 10
            
            # Sample rate score (optimal: >= 22050)
            if sr >= 44100:
                score += 25
            elif sr >= 22050:
                score += 20
            else:
                score += 10
            
            # Energy score
            rms_energy = quality_metrics['audio_metrics']['rms_energy']
            if 0.01 <= rms_energy <= 0.5:
                score += 25
            else:
                score += 15
            
            # Spectral score
            spectral_centroid = quality_metrics['spectral_metrics']['spectral_centroid_mean']
            if 1000 <= spectral_centroid <= 4000:
                score += 25
            else:
                score += 15
            
            quality_metrics['overall_score'] = min(100.0, score)
            
            # Quality level
            if quality_metrics['overall_score'] >= 80:
                quality_level = "Excellent"
            elif quality_metrics['overall_score'] >= 60:
                quality_level = "Good"
            elif quality_metrics['overall_score'] >= 40:
                quality_level = "Fair"
            else:
                quality_level = "Poor"
            
            quality_metrics['quality_level'] = quality_level
            
            print(f"üîç Voice quality assessment for {voice_id}:")
            print(f"üìä Overall Score: {quality_metrics['overall_score']:.1f}/100 ({quality_level})")
            print(f"‚è±Ô∏è Duration: {quality_metrics['duration']:.2f}s")
            print(f"üéµ Sample Rate: {sr} Hz")
            print(f"‚ö° RMS Energy: {quality_metrics['audio_metrics']['rms_energy']:.4f}")
            
            return quality_metrics
            
        except Exception as e:
            print(f"‚ùå Error assessing voice quality: {e}")
            return {"error": str(e)}

    def transform_voice(self, voice_id: str, transformation_type: str, 
                       intensity: float = 0.5, output_path: str = None) -> str:
        """
        Bi·∫øn ƒë·ªïi voice sample v·ªõi c√°c hi·ªáu ·ª©ng ƒë·∫∑c bi·ªát
        
        Args:
            voice_id: ID c·ªßa voice c·∫ßn bi·∫øn ƒë·ªïi
            transformation_type: Lo·∫°i bi·∫øn ƒë·ªïi
            intensity: C∆∞·ªùng ƒë·ªô bi·∫øn ƒë·ªïi (0.0 - 1.0)
            output_path: ƒê∆∞·ªùng d·∫´n output (optional)
            
        Returns:
            ƒê∆∞·ªùng d·∫´n ƒë·∫øn file audio ƒë√£ bi·∫øn ƒë·ªïi
        """
        if voice_id not in self.voice_samples:
            raise ValueError(f"Voice ID '{voice_id}' not found. Please add voice sample first.")
        
        # Validate transformation type
        valid_transformations = {
            'robot': 'Robot voice effect',
            'alien': 'Alien voice effect', 
            'monster': 'Monster voice effect',
            'chipmunk': 'Chipmunk voice effect',
            'giant': 'Giant voice effect',
            'whisper': 'Whisper voice effect',
            'radio': 'Radio/telephone effect',
            'underwater': 'Underwater effect',
            'space': 'Space/echo effect',
            'time_warp': 'Time warp effect'
        }
        
        if transformation_type not in valid_transformations:
            raise ValueError(f"Invalid transformation type. Choose from: {list(valid_transformations.keys())}")
        
        # Validate intensity
        if intensity < 0.0 or intensity > 1.0:
            print(f"‚ö†Ô∏è Warning: Intensity {intensity} out of range [0.0, 1.0], using 0.5")
            intensity = 0.5
        
        try:
            import librosa
            import soundfile as sf
            import numpy as np
            
            voice_info = self.voice_samples[voice_id]
            audio_path = voice_info['audio_path']
            
            if not os.path.exists(audio_path):
                raise FileNotFoundError("Audio file not found")
            
            # Load audio
            audio, sr = librosa.load(audio_path, sr=None)
            
            # Apply transformation based on type
            transformed_audio = audio.copy()
            
            if transformation_type == 'robot':
                # Robot effect: subtle metallic harmonics + pitch shift
                transformed_audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=intensity * 2)
                # Very subtle harmonics
                harmonics = np.sin(2 * np.pi * 150 * np.arange(len(audio)) / sr) * intensity * 0.1
                transformed_audio += harmonics
                
            elif transformation_type == 'alien':
                # Alien effect: pitch shift only (no harmonics)
                transformed_audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=intensity * 6)
                
            elif transformation_type == 'monster':
                # Monster effect: lower pitch only (no growl)
                transformed_audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=-intensity * 4)
                
            elif transformation_type == 'chipmunk':
                # Chipmunk effect: higher pitch only
                transformed_audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=intensity * 6)
                
            elif transformation_type == 'giant':
                # Giant effect: lower pitch + subtle slow down
                transformed_audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=-intensity * 3)
                if intensity > 0.5:
                    transformed_audio = librosa.effects.time_stretch(transformed_audio, rate=1.0 - intensity * 0.3)
                
            elif transformation_type == 'whisper':
                # Whisper effect: reduce energy only (no breath noise)
                transformed_audio = audio * (1.0 - intensity * 0.5)
                
            elif transformation_type == 'radio':
                # Radio effect: bandpass filter only (no noise)
                low_freq = 300
                high_freq = 3000
                freqs = np.fft.fftfreq(len(audio), 1/sr)
                mask = (freqs >= low_freq) & (freqs <= high_freq)
                fft_audio = np.fft.fft(audio)
                fft_audio[~mask] *= intensity * 0.5
                transformed_audio = np.real(np.fft.ifft(fft_audio))
                
            elif transformation_type == 'underwater':
                # Underwater effect: low-pass filter only (no bubbles)
                cutoff_freq = 1000 + intensity * 2000
                freqs = np.fft.fftfreq(len(audio), 1/sr)
                mask = np.abs(freqs) <= cutoff_freq
                fft_audio = np.fft.fft(audio)
                fft_audio[~mask] *= intensity * 0.4
                transformed_audio = np.real(np.fft.ifft(fft_audio))
                
            elif transformation_type == 'space':
                # Space effect: subtle echo only (no cosmic sounds)
                delay_samples = int(sr * intensity * 0.3)
                echo = np.zeros_like(audio)
                echo[delay_samples:] = audio[:-delay_samples] * intensity * 0.3
                transformed_audio += echo
                
            elif transformation_type == 'time_warp':
                # Time warp effect: gentle speed variation
                warp_points = int(intensity * 5) + 1
                segment_length = len(audio) // warp_points
                warped_audio = []
                
                for i in range(warp_points):
                    start = i * segment_length
                    end = start + segment_length
                    segment = audio[start:end]
                    
                    # Gentle speed variation
                    speed_factor = 0.8 + intensity * 0.4  # Range: 0.8 - 1.2
                    warped_segment = librosa.effects.time_stretch(segment, rate=speed_factor)
                    warped_audio.append(warped_segment)
                
                transformed_audio = np.concatenate(warped_audio)
                # Trim to original length
                if len(transformed_audio) > len(audio):
                    transformed_audio = transformed_audio[:len(audio)]
                else:
                    # Pad if shorter
                    padding = np.zeros(len(audio) - len(transformed_audio))
                    transformed_audio = np.concatenate([transformed_audio, padding])
            
            # Clean up audio: remove noise and artifacts
            # Apply gentle noise gate to remove very low amplitude noise
            noise_threshold = 0.01 * intensity
            transformed_audio[np.abs(transformed_audio) < noise_threshold] = 0
            
            # Apply gentle compression to smooth out dynamics
            transformed_audio = np.tanh(transformed_audio * 0.8)
            
            # Normalize audio
            transformed_audio = librosa.util.normalize(transformed_audio)
            
            # Generate output path
            if not output_path:
                output_path = f"transformed_{voice_id}_{transformation_type}_{intensity:.2f}.wav"
            
            # Save transformed audio
            sf.write(output_path, transformed_audio, sr)
            
            print(f"üé≠ Voice transformation completed!")
            print(f"üîß Type: {transformation_type} ({valid_transformations[transformation_type]})")
            print(f"‚ö° Intensity: {intensity:.2f}")
            print(f"üìÅ Output: {output_path}")
            
            return output_path
            
        except Exception as e:
            print(f"‚ùå Error in voice transformation: {e}")
            raise


# Utility functions
def validate_audio_file(audio_path: str) -> bool:
    """Ki·ªÉm tra file audio c√≥ h·ª£p l·ªá kh√¥ng"""
    try:
        info = sf.info(audio_path)
        # Ki·ªÉm tra sample rate (n√™n l√† 22050Hz cho XTTS)
        if info.samplerate != 22050:
            print(f"‚ö†Ô∏è Warning: Audio sample rate is {info.samplerate}Hz, recommended: 22050Hz")
        
        # Ki·ªÉm tra duration (n√™n t·ª´ 3-10 gi√¢y)
        if info.duration < 2 or info.duration > 15:
            print(f"‚ö†Ô∏è Warning: Audio duration is {info.duration:.1f}s, recommended: 3-10s")
        
        return True
    except Exception as e:
        print(f"‚ùå Invalid audio file: {e}")
        return False


def convert_audio_format(input_path: str, output_path: str, target_sr: int = 22050):
    """Chuy·ªÉn ƒë·ªïi format audio sang WAV v·ªõi sample rate mong mu·ªën"""
    try:
        import librosa
        
        # Load audio
        audio, sr = librosa.load(input_path, sr=target_sr)
        
        # Save as WAV
        sf.write(output_path, audio, target_sr)
        
        print(f"‚úÖ Audio converted: {input_path} -> {output_path}")
        return True
        
    except Exception as e:
        print(f"‚ùå Error converting audio: {e}")
        return False


if __name__ == "__main__":
    # Example usage
    cloner = VoiceCloner()
    
    # Th√™m voice sample
    cloner.add_voice_sample("my_voice", "path/to/audio.wav", "Xin ch√†o")
    
    # Clone voice c∆° b·∫£n
    output = cloner.clone_voice("ƒê√¢y l√† gi·ªçng n√≥i ƒë√£ clone!", "my_voice")
    print(f"Basic output: {output}")
    
    # Clone voice v·ªõi ng√¥n ng·ªØ c·ª• th·ªÉ
    output_en = cloner.clone_voice("Hello, this is cloned voice!", "my_voice", language="en")
    print(f"English output: {output_en}")
    
    # Clone voice v·ªõi audio effects ƒë·∫ßy ƒë·ªß
    output_effects = cloner.clone_voice_with_effects(
        "Xin ch√†o v·ªõi hi·ªáu ·ª©ng!", 
        "my_voice", 
        speed=1.2, 
        pitch_shift=2.0,
        voice_type="female",
        age_group="teen"
    )
    print(f"Effects output: {output_effects}")
    
    # Demo auto-language detection
    texts = [
        "Hello world",  # English
        "Bonjour le monde",  # French
        "Hola mundo",  # Spanish
        "„Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå",  # Japanese
        "ÏïàÎÖïÌïòÏÑ∏Ïöî ÏÑ∏Í≥Ñ",  # Korean
        "‰Ω†Â•Ω‰∏ñÁïå",  # Chinese
        "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡πÇ‡∏•‡∏Å",  # Thai
        "Xin ch√†o th·∫ø gi·ªõi",  # Vietnamese - Special handling
        "Halo dunia",  # Indonesian
        "Selamat pagi dunia",  # Malay
    ]
    
    for text in texts:
        detected_lang = cloner.auto_detect_language(text)
        print(f"Text: {text} -> Language: {detected_lang}")
    
    # Demo Vietnamese voice cloning
    print("\nüáªüá≥ Vietnamese Voice Cloning Demo:")
    try:
        vietnamese_output = cloner.clone_voice(
            "Xin ch√†o! T√¥i l√† gi·ªçng n√≥i ti·∫øng Vi·ªát ƒë∆∞·ª£c clone t·ª´ Coqui TTS.", 
            "my_voice", 
            language="vi"
        )
        print(f"‚úÖ Vietnamese clone successful: {vietnamese_output}")
    except Exception as e:
        print(f"‚ùå Vietnamese clone failed: {e}")
    
    # Demo voice types
    voice_types = ["normal", "male", "female", "child", "elderly"]
    age_groups = ["child", "teen", "adult", "elderly"]
    
    print("\nüé≠ Voice Types:", voice_types)
    print("üë• Age Groups:", age_groups)
    print("‚ö° Speed Range: 0.5x - 2.0x")
    print("üéµ Pitch Range: -12 to +12 semitones")
    print("üìù Text Limit: 500 characters")
    print(f"üåç Supported Languages: {len(cloner.SUPPORTED_LANGUAGES)} languages")
    print("üáªüá≥ Vietnamese: Special handling with English base language") 

    # Example of voice comparison
    print("\nüîç Voice Comparison Demo:")
    try:
        comparison_results = cloner.compare_voices(
            "my_voice", "my_voice", "Hello, this is a voice comparison test. How do I sound?"
        )
        print(f"Comparison results: {json.dumps(comparison_results, indent=2)}")
    except Exception as e:
        print(f"‚ùå Voice comparison failed: {e}") 

    # Example of real-time voice cloning
    print("\nüîÑ Real-time Voice Cloning Demo:")
    try:
        # Create a generator for text chunks
        def text_generator():
            yield "Hello, this is a real-time voice cloning test."
            yield "This is the second chunk of the real-time test."
            yield "And this is the final chunk."

        # Stream the voice cloning
        for result in cloner.stream_voice_cloning(text_generator(), "my_voice"):
            if 'error' in result:
                print(f"Error in stream: {result['error']}")
            else:
                print(f"Chunk {result['chunk_id']}: Audio size {result['audio_size']} bytes")
    except Exception as e:
        print(f"‚ùå Real-time voice cloning failed: {e}") 